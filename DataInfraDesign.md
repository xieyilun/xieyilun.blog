# 基于分布式有序队列的数据架构设计

本人最近几年在推荐架构内做kv存储相关的工作，工作中涉及到许多自研的分布式存储项目以及rocksdb、redis、kafka、zookeeper等常用开源组件。绝大部分可靠的行存储系统，都依赖一个有序队列，用来做重启数据恢复或者分布式数据同步，但通常每个存储系统都只关注自身需求，设计的有序队列只是刚好满足自身需要，这反而限制了系统的扩展性。推荐系统中涉及的数据绝大部分都是流式更新的，消息队列连通了推荐系统的绝大部分模块，本文希望设计出一个通用的分布式有序队列，并基于此改进几个常见的基础组件，从而构建一套基于分布式有序队列的数据流动、存储架构。本文提及的大部分功能现在散落在我们公司的各个自研组件中，整体的设计只是本人的设想，并没有落地。

### 通用分布式有序队列

基于paxos或者raft的分布式有序队列，数据同步、提交本地状态机这些基本功能本文就不再赘述了

learner：paxos的这个角色一般没什么存在感，但其实是可以有非常多的应用场景的，比较典型的应用就是zookeeper的observer。我这里需要给它增加一个learner间树形转发消息的能力，具体作用后面会举例。

指定upsteam：learner指定转发的上游，可以是其它learner或者accepter

Filter：learner可能只关心部分消息，上游可以针对不同的leaner做个性过滤，只发送learner关心的消息，甚至可以截断、修改消息内容，只发送下游想要的内容

### 元数据服务
Zookeeper是一个广泛使用并且大家普遍对它不满的服务，主要问题有：性能差、接口丑、功能不够。
我们自研的kv存储服务算是对zookeeper使用比较重的场景，它的接口我基本都用上了，我这里就根据自己的使用经验做出一些改进的设计。

由于zk每个集群全局只有一个有序队列，所以写入相当于是单线程的，但实际使用中，一个zk集群往往会存储多块互不相干的数据，这些数据共用一个队列显然是不合理的。zk的目录结构可以看成是一棵树，现实中许多zk集群，是可以把这颗树划分成互不相干的若干棵子树的，让这些子树各自拥有独立的操作队列，这些子树的祖先节点，不允许有值，只能创建和删除。

现在zk的每个watch，是注册到每个节点上的，如果需要watch的位置比较多，实现会比较复杂而且效率不高。其实客户端也可以作为一个observer，绑定一个子树，同步操作队列，在本地缓存这个子树，这样每个位置的回调只需要在本地设置，server端不需要保存一大堆watch信息了，而且使用observer模式就可以避免丢消息的问题。

### 消息队列服务
一个accepter不挂状态机的分布式有序队列，天然就是一个消息队列，consumer就是learner。我所在的推荐架构，有许多服务的数据分发压力非常大，kafka远远无法满足需求，所以公司自研了一个consumer自带数据分发能力的消息队列组件。只要learner直接提供这个能力，就可以直接基于分布式有序队列实现一个拥有强大分发能力的消息队列服务了。

### 流式计算
常规的流式计算，一般是接一个消息队列作为数据源，计算后将结果输出到一个新的消息队列或者存储，不论是消息队列还是存储，一般都是多副本的，并且需要单独申请资源。在一个较长的数据链路中，许多中间链路往往是将数据简单处理，为这些中间链路单独分配多副本的存储资源会觉得浪费资源，但出于架构上的各种原因，又不得不把中间链路独立出来。对于轻量的中间计算，其实可以不必主动消费上游再把结果输出到确定的下游，而是可以做成一个现算的消息队列，当下游来拉取数据时，按照seq_num现场去上游拉取数据，现场计算并输出结果。这样做一是可以节省中间链路的存储资源，二是可以让整个数据流的seq_num对齐，整个数据流上的各个模块如果能基于一个对齐的seq_num，那将给整个系统的设计增加非常多的可能性，三是在一套系统内就维护了明确的上下游关系，可以非常容易地建出整个链路的拓扑图。

### 存储
推荐系统中需要大量使用kv或者说nosql存储，但开源的项目一般在功能和性能上都无法满足较大规模的推荐系统的需求，在其它系统内应该也有类似的问题，所以业务达到一定规模的公司基本都有自研nosql。但就如本文开头提到的，存储系统使用的有序队列往往是封闭的，这其实限制了许多可能性，下面我就给出一个使用有序队列连接多个模块的存储系统的设计。

持久化的部分，基于paxos或者raft构建可靠的kv存储的方案已经很常见了，这里就不赘述了。在我维护的kv存储项目中，持久化的kv存储虽然在延迟方面达到了接近内存kv的水平，但cpu的开销高达内存kv的5到10倍，所以需要增加一层内存kv的缓存，用来提供更高的读qps。这种情况下，持久化和内存的实例，可以基于同一条有序队列，持久化实例存储全量数据，内存实例只保留热点数据，这样就可以解决缓存和持久化的数据同步问题。

在有些场景下，远程的缓存服务性能仍不能满足需求，使用方需要在本地缓存部分热点数据。简单的做法是每隔一段时间就废弃掉本地缓存，等下次miss再去存储服务去取，但这就会有不小的时效性损失。如果使用方本身也作为一个learner，去订阅存储服务的有序队列，就可以解决时效性的问题。但使用方本地缓存的热点数据，一般都只占整个存储的很小一部分，订阅完整的队列会造成大量的浪费，此时需要存储server能够把热点的key挑选出来，在给使用方发送它订阅的数据流时，过滤掉非热点数据，再加上各个主调方作为learner时，会内部分发数据，这样就可以大幅降低存储服务的压力，同时不牺牲时效性。

![存储架构.png](https://github.com/xieyilun/xieyilun.blog/blob/main/%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84.png)

上述架构只是基于典型场景举的例子，本文的核心思想是希望通过一套通用、开放的有序队列，把复杂系统内的各个模块更简单、优雅地连接起来

### 依赖关系

![依赖关系.png](https://github.com/xieyilun/xieyilun.blog/blob/main/%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB.png)
